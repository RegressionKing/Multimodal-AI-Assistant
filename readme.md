# Multimodal AI Assistant Image processing with Whisper and Llava


Whisper is your intelligent companion, designed to assist you with a wide range of tasks using advanced multimodal AI capabilities. From voice commands to image recognition, Whisper is here to make your life easier and more efficient.
Integrated with Llava-7B makes it a perfect assistant for daily use.

## Features

- **Voice Commands**: Interact with Whisper using natural language voice commands.
- **Image Recognition**: Utilize image recognition capabilities to identify objects, scenes, and more.
- **Text-to-Speech**: Convert text input into natural-sounding speech output for seamless communication.
- **Speech-to-Text**: Transform spoken words into text for processing and analysis.
- **Personal Assistant**: Use your voice to give commands now for your image.
- **User-Friendly Interface**: Enjoy a user-friendly interface powered by Gradio for intuitive interactions.


## Getting Started

To get started, follow these simple steps:

1. **Clone the Repository**: Clone this GitHub repository to your local machine.

```bash
git clone https://github.com/CoderOMaster/Multimodal-AI-Assistant.git
```

2. **Install Dependencies**: Navigate to the project directory and install the required dependencies.

```bash
cd Multimodal-AI-Assistant
pip install -r requirements.txt
```

3. **Run the Application**: Run the application using jupyter notebook or collab and run all cells.

```bash
jupyter notebook
```

4. **Start Interacting**: Once the application is running, start interacting with Whisper using voice commands or other input methods.Use Google Collab for best experience since bitsandbytes library often troubles users deploying it locally.



## Contributing

Contributions are welcome! If you have any ideas for new features, improvements, or bug fixes, feel free to open an issue or submit a pull request.
